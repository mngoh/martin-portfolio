<!DOCTYPE html>
<html>
<head>
  <title>Martin N.</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="shortcut icon" type="image/x-icon" href="../logo/favicon.ico" />
  <link rel="stylesheet" type="text/css" href="../css/style.css">
  <link rel="stylesheet" type="text/css" href="../css/copyright.css">
  <link rel="stylesheet" type="text/css" href="../css/blog.css">
  <script src="../js/script.js" defer></script>
  <script src="https://kit.fontawesome.com/b2fb9e30a3.js" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat&display=swap">
</head>
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6402562962566604"
      crossorigin="anonymous"></script>
<body>
  <!-- <header class="head-name" onclick="window.location.href = '../blog.html';">Martin Ngoh</header> -->
  <nav class="navbar-blog">
      <!-- <div class="name"><img src="logo/MN.png" style="height: rem;"> </div> -->
      <a href="#" class="toggle-button" onclick='toggleNavbarBackground()'>
        <span class="bar"></span>
        <span class="bar"></span>
        <span class="bar"></span>
      </a>
      <div class ="navbar-links">
        <ul>
          <li><a href="#Home" onclick="window.location.href = '../blog.html';">Home</a></li>
          <li><a href="#Ai" onclick="window.location.href = '../index.html';">About</a></li>
        </ul>
      </div>
      <div class="name">Martin Ngoh </div>
      <ul class="footer-links">
        <li><a href="mailto:contact@martinngoh.com"><i class="fa-regular fa-envelope fa-2xl" id="mail"></i></a></li>
        <li><a href="https://www.linkedin.com/in/mngoh"><i class="fa-brands fa-linkedin fa-2xl"></i></a></li>
        <li><a href="https://github.com/mngoh"><i class="fa-brands fa-github fa-2xl" id="github"></i></a></li>
        <li><a href="https://www.instagram.com/martinngoh"><i class="fa-brands fa-instagram fa-2xl"></i></a></li>
      </ul>
    </nav>
    <div class="container">
      <h1>What is Gradient Boosting?</h1>
      <p>Gradient Boosting is a fundamental machine learning (ML) technique that enhances the accuracy of decision trees by sequentially adding more trees.
        Gradient Boosting is a framework that is key in understanding how popular ML Algorithms like XGBoost work. 
      </p>
      <a href="#GBM">Click Here to see the XGBoost Example in Python</a>
      
      <h2>Importance of Gradient Boosting</h2>
      <ul>
          <li>It creates a more robust model compared to a single decision tree.</li>
      </ul>
      <h2>Types of Gradient Boosting Algorithms</h2>

      <h3>Gradient Boosting Machine (GBM)</h3>
      <p>GBM is the original gradient boosting algorithm that was created by Jerome Friedman. The 
        algorithm works by building a sequence of trees where each new tree aims to correct the errors made by the prior ones. 
        The GBM algorithm minimizes a loss function using gradient descent.
      </p>

      <h3>XGBoost (Extreme Gradient Boosting)</h3>
      <p>XGBoost is a popular gradient boosting technique created by Tianqi Chen. This algorithm is an enhanced version of GBM by employing 
        parallell tree construction, and regularization. XGBoost gained popularity due to its speed and performance. 
      </p>
      
      <h3>CatBoost</h3>
      <p>CatBoost is a gradient boosting library that was created by Yandex and designed to handle large amounts of categorical variables. 
        The algorithm uses regularization techniques in attempt to avoid overfitting.
      </p>

      <h3>LightGBM</h3>
      <p>LightGBM is another gradient boosting framework that was developed by Microsoft. This algorithm introduces Gradient-based One-Side Sampling (GOSS) and Exclusive 
        Feature Bundling (EFB). These concepts are optimization techniques that were added to improve the efficiency and effectiveness of training gradient boosting models for large datasets. 
        GOSS works by applying a down-sampling strategy vs sampling uniformly across the dataset. This decreases computation costs. 
        EFB works by allowing the model to capture interactions among similar feature values, reducing dimensions.
      </p>

      <h2> Pros and Cons of Gradient Boosting</h2>
      <h3>Pros</h3>
      <li>High Accuracy - Because of the combination of decision trees, gradient boosting algorithms have the ability to predict with high accuracy.</li>
      <li>Data - Gradient boosting algorithms allow for a wide range of data types to be used in the algorithms, effectively handling various data.</li>
      <li>Robust - Regularization techniques ensure that gradient boosting models are robust to overfitting. </li>

      <h3>Cons</h3>
      <li>Computationally Expensive - Gradient boosting on large datasets can be very computationally expensive due to the sequential nature of the algorithm.</li>
      <li>Sensitive - The performance of a gradient boosting algorithm is very sensitive to hyperparameter tuning making it a challenging model to optimize.</li>
      <li> Interpretability -  </li>
      
      <h2 id="GBM">Gradient Boosting with XGBoost in Python</h2>
      <p>Benefits and usage of XGBoost in Gradient Boosting.</p>
      
      <h2>Get Started with Gradient Boosting Today</h2>
      <p>Encouragement to delve deeper into Gradient Boosting and its applications.</p>
  </div>
</body>
</html>
